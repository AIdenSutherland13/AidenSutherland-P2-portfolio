[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Installing Data from GitHub\n\n# Clean data provided by &lt;https://github.com/kkakey/American_Idol&gt;. No cleaning was necessary.\nauditions &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/auditions.csv\")\n\nRows: 142 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): audition_city, audition_venue, episodes, episode_air_date, callbac...\ndbl  (2): season, tickets_to_hollywood\ndate (4): audition_date_start, audition_date_end, callback_date_start, callb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\neliminations &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/elimination_chart.csv\")\n\nRows: 456 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): place, gender, contestant, top_36, top_36_2, top_36_3, top_36_4, t...\ndbl  (1): season\nlgl  (1): comeback\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfinalists &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/finalists.csv\")\n\nRows: 190 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Contestant, Birthday, Birthplace, Hometown, Description\ndbl (1): Season\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/ratings.csv\")\n\nRows: 593 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): episode, airdate, 18_49_rating_share, timeslot_et, dvr_18_49, dvr_...\ndbl  (4): season, show_number, viewers_in_millions, nightlyrank\nlgl  (1): ref\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nseasons &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/seasons.csv\")\n\nRows: 18 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): winner, runner_up, original_release, original_network, hosted_by, j...\ndbl (2): season, no_of_episodes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsongs &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/Songs/songs_all.csv\")\n\nRows: 2429 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): season, week, contestant, song, artist, song_theme, result\ndbl (1): order\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nView the first few rows of each dataset\n\nhead(auditions)\n\n# A tibble: 6 × 12\n  season audition_date_start audition_date_end audition_city      audition_venue\n   &lt;dbl&gt; &lt;date&gt;              &lt;date&gt;            &lt;chr&gt;              &lt;chr&gt;         \n1      1 2002-04-20          2002-04-22        Los Angeles, Cali… Westin Bonave…\n2      1 2002-04-23          2002-04-25        Seattle, Washingt… Hyatt Regency…\n3      1 2002-04-26          2002-04-28        Chicago, Illinois  Congress Plaz…\n4      1 2002-04-29          2002-05-01        New York City, Ne… Millenium Hil…\n5      1 2002-05-03          2002-05-05        Atlanta, Georgia   AmericasMart/…\n6      1 2002-05-05          2002-05-07        Dallas, Texas      Wyndham Anato…\n# ℹ 7 more variables: episodes &lt;chr&gt;, episode_air_date &lt;chr&gt;,\n#   callback_venue &lt;chr&gt;, callback_date_start &lt;date&gt;, callback_date_end &lt;date&gt;,\n#   tickets_to_hollywood &lt;dbl&gt;, guest_judge &lt;chr&gt;\n\nhead(eliminations)\n\n# A tibble: 6 × 46\n  season place gender contestant        top_36 top_36_2 top_36_3 top_36_4 top_32\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1      1 1     Female Kelly Clarkson    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n2      1 2     Male   Justin Guarini    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n3      1 3     Female Nikki McKibbin    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n4      1 4     Female Tamyra Gray       &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n5      1 5     Male   R. J. Helton      &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n6      1 6     Female Christina Christ… &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n# ℹ 37 more variables: top_32_2 &lt;chr&gt;, top_32_3 &lt;chr&gt;, top_32_4 &lt;chr&gt;,\n#   top_30 &lt;chr&gt;, top_30_2 &lt;chr&gt;, top_30_3 &lt;chr&gt;, top_25 &lt;chr&gt;, top_25_2 &lt;chr&gt;,\n#   top_25_3 &lt;chr&gt;, top_24 &lt;chr&gt;, top_24_2 &lt;chr&gt;, top_24_3 &lt;chr&gt;, top_20 &lt;chr&gt;,\n#   top_20_2 &lt;chr&gt;, top_16 &lt;chr&gt;, top_14 &lt;chr&gt;, top_13 &lt;chr&gt;, top_12 &lt;chr&gt;,\n#   top_11 &lt;chr&gt;, top_11_2 &lt;chr&gt;, wildcard &lt;chr&gt;, comeback &lt;lgl&gt;, top_10 &lt;chr&gt;,\n#   top_9 &lt;chr&gt;, top_9_2 &lt;chr&gt;, top_8 &lt;chr&gt;, top_8_2 &lt;chr&gt;, top_7 &lt;chr&gt;,\n#   top_7_2 &lt;chr&gt;, top_6 &lt;chr&gt;, top_6_2 &lt;chr&gt;, top_5 &lt;chr&gt;, top_5_2 &lt;chr&gt;, …\n\nhead(finalists)\n\n# A tibble: 6 × 6\n  Contestant          Birthday  Birthplace           Hometown Description Season\n  &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;                &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n1 Kelly Clarkson      24-Apr-82 Fort Worth, Texas    Burleso… \"She perfo…      1\n2 Justin Guarini      28-Oct-78 Columbus, Georgia    Doylest… \"He perfor…      1\n3 Nikki McKibbin      28-Sep-78 Grand Prairie, Texas &lt;NA&gt;     \"She had p…      1\n4 Tamyra Gray         26-Jul-79 Takoma Park, Maryla… Atlanta… \"She had a…      1\n5 R. J. Helton        17-May-81 Pasadena, Texas      Cumming… \"J. Helton…      1\n6 Christina Christian 21-Jun-81 Brooklyn, New York   &lt;NA&gt;     \".Christin…      1\n\nhead(ratings)\n\n# A tibble: 6 × 17\n  season show_number episode    airdate `18_49_rating_share` viewers_in_millions\n   &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n1      1           1 Auditions  June 1… 4.8                                 9.85\n2      1           2 Hollywood… June 1… 5.2                                11.2 \n3      1           3 Top 30: G… June 1… 5.2                                10.3 \n4      1           4 Top 30: G… June 1… 4.7                                 9.47\n5      1           5 Top 30: G… June 2… 4.5                                 9.08\n6      1           6 Top 30: G… June 2… 4.2                                 8.53\n# ℹ 11 more variables: timeslot_et &lt;chr&gt;, dvr_18_49 &lt;chr&gt;,\n#   dvr_viewers_millions &lt;chr&gt;, total_18_49 &lt;chr&gt;,\n#   total_viewers_millions &lt;chr&gt;, weekrank &lt;chr&gt;, ref &lt;lgl&gt;, share &lt;chr&gt;,\n#   nightlyrank &lt;dbl&gt;, rating_share_households &lt;chr&gt;, rating_share &lt;chr&gt;\n\nhead(seasons)\n\n# A tibble: 6 × 10\n  season winner     runner_up original_release original_network hosted_by judges\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt; \n1      1 Kelly Cla… Justin G… June 11 (2002-0… Fox              Ryan Sea… Paula…\n2      2 Ruben Stu… Clay Aik… January 21 (200… Fox              Ryan Sea… Paula…\n3      3 Fantasia … Diana De… January 19 (200… Fox              Ryan Sea… Paula…\n4      4 Carrie Un… Bo Bice   January 18 (200… Fox              Ryan Sea… Paula…\n5      5 Taylor Hi… Katharin… January 17 (200… Fox              Ryan Sea… Paula…\n6      6 Jordin Sp… Blake Le… January 16 (200… Fox              Ryan Sea… Paula…\n# ℹ 3 more variables: no_of_episodes &lt;dbl&gt;, finals_venue &lt;chr&gt;, mentor &lt;chr&gt;\n\nhead(songs)\n\n# A tibble: 6 × 8\n  season    week                 order contestant song  artist song_theme result\n  &lt;chr&gt;     &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; \n1 Season_01 20020618_top_30_gro…     1 Tamyra Gr… And … Jenni… &lt;NA&gt;       Advan…\n2 Season_01 20020618_top_30_gro…     2 Jim Verra… When… Doris… &lt;NA&gt;       Advan…\n3 Season_01 20020618_top_30_gro…     3 Adriel He… I'll… Edwin… &lt;NA&gt;       Elimi…\n4 Season_01 20020618_top_30_gro…     4 Rodesia E… Dayd… The M… &lt;NA&gt;       Elimi…\n5 Season_01 20020618_top_30_gro…     5 Natalie B… Crazy Patsy… &lt;NA&gt;       Elimi…\n6 Season_01 20020618_top_30_gro…     6 Brad Estr… Just… James… &lt;NA&gt;       Elimi…\n\n\nData cleaning/wrangling - I converted every chr column to a factor so I can easily plot them if needed to. The only dataset that I removed NA from was the songs dataset.\n\n# Load necessary libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(ggcorrplot)\n\nWarning: package 'ggcorrplot' was built under R version 4.3.3\n\n# Convert relevant columns to appropriate data types\nauditions &lt;- auditions %&gt;%\n  mutate(across(where(is.character), as.factor))\n\neliminations &lt;- eliminations %&gt;%\n  mutate(across(where(is.character), as.factor))\n\nfinalists &lt;- finalists %&gt;%\n  mutate(across(where(is.character), as.factor))\n\nratings &lt;- ratings %&gt;%\n  mutate(across(where(is.character), as.factor))\n\nseasons &lt;- seasons %&gt;%\n  mutate(across(where(is.character), as.factor))\n\nsongs &lt;- songs %&gt;%\n  mutate(across(where(is.character), as.factor))\nsongs &lt;- songs %&gt;%\n  drop_na()\nsongs$season &lt;- as.numeric(songs$season)\n\ndfs &lt;- list(auditions, eliminations, finalists, ratings, seasons, songs)\n\n# Remove duplicates from each dataframe\ndfs_unique &lt;- lapply(dfs, function(df) {\n  df %&gt;%\n    distinct()\n})\n\n\nnames(finalists)[names(finalists) == \"Contestant\"] &lt;- \"contestant\"\nnames(finalists)[names(finalists) == \"Season\"] &lt;- \"season\"\n\n\ndata_combined &lt;- songs %&gt;%\n  inner_join(finalists, by = c(\"season\", \"contestant\"), relationship = \"many-to-many\")\ndata_combined &lt;- data_combined %&gt;%\n  left_join(seasons, by = c(\"season\"), relationship = \"many-to-many\")\ndata_combined &lt;- data_combined %&gt;%\n  inner_join(ratings, by = \"season\", relationship = \"many-to-many\")\n\nI want to see what effects the overall viewers in millions, below is for the numeric variables only.To determine the categorical variables I will be going through visualizations.\n\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nnumeric_vars &lt;- data_combined %&gt;% select_if(is.numeric)\ncor_matrix &lt;- cor(numeric_vars, use = \"complete.obs\")\n\nWarning in cor(numeric_vars, use = \"complete.obs\"): the standard deviation is\nzero\n\n# Extract correlations with 'viewers_in_millions'\ncor_viewers &lt;- cor_matrix[\"viewers_in_millions\", ]\ncor_viewers &lt;- sort(cor_viewers, decreasing = TRUE)\nprint(cor_viewers)\n\nviewers_in_millions               order         show_number         nightlyrank \n       1.000000e+00       -1.066709e-20       -1.893358e-01       -4.447735e-01 \n\n\n\n#Box Plot of Viewers in Millions by week\nggplot(data_combined, aes(x = week, y = viewers_in_millions)) +\n  geom_boxplot() +\n  facet_wrap(~season, scales = \"free\")+\n  ggtitle(\"Box Plot of Viewers in Millions by week\")\n\nWarning: Removed 118 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Box Plot of Viewers in Millions by Contestant\nggplot(data_combined, aes(x = contestant, y = viewers_in_millions)) +\n  geom_boxplot() + \n  facet_wrap(~season, scales = \"free\")+\n  ggtitle(\"Box Plot of Viewers in Millions by Contestant\")\n\nWarning: Removed 118 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Box Plot of Viewers in Millions by Song\nggplot(data_combined, aes(x = song, y = viewers_in_millions)) +\n  geom_boxplot() + \n  facet_wrap(~season, scales = \"free\")+\n  ggtitle(\"Box Plot of Viewers in Millions by Song\")\n\nWarning: Removed 118 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Box Plot of Viewers in Millions by Finals_venue\nggplot(data_combined, aes(x = finals_venue, y = viewers_in_millions)) +\n  geom_boxplot() + \n  ggtitle(\"Box Plot of Viewers in Millions by Finals_venue\")\n\nWarning: Removed 118 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Box Plot of Viewers in Millions by Time Slot (ET), also removing all timeslots with NA\ndf &lt;- data_combined %&gt;%\n  filter(!is.na(timeslot_et))\n\nggplot(df, aes(x = timeslot_et, y = viewers_in_millions)) +\n  geom_boxplot() + \n  ggtitle(\"Box Plot of Viewers in Millions by Time Slot (ET)\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n#Box Plot of Viewers in Millions by Week Rank\nggplot(data_combined, aes(x = weekrank, y = viewers_in_millions)) +\n  geom_boxplot() + \n  ggtitle(\"Box Plot of Viewers in Millions by Week Rank\")\n\nWarning: Removed 118 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n#Box Plot of Viewers in Millions by Original Network\nggplot(data_combined, aes(x = original_network, y = viewers_in_millions)) +\n  geom_boxplot() + \n  ggtitle(\"Box Plot of Viewers in Millions by Original Network\")\n\nWarning: Removed 118 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nIt appears that original network, week_rank, timeslot_et, and finals_venue all show a pattern when pertaining to viewers in millions. The only numerical predictor that shows a negative relationship between views is nightly rank.\nBased on my initial findings my hypothesis is that nightly rank, original network, week_rank, timeslot_et, and finals_venue all have an effect on the overall views one episode can get.\n\nselected_col &lt;- c(\"original_network\", \"weekrank\", \"timeslot_et\", \"finals_venue\", \"nightlyrank\", \"viewers_in_millions\")\n\nnew_data &lt;- data_combined %&gt;%\n  select(all_of(selected_col))\n\nImpute missing values in numerical columns\n\ndata &lt;- new_data %&gt;%\n  mutate(nightlyrank = ifelse(is.na(nightlyrank), median(nightlyrank, na.rm = TRUE), nightlyrank))\n\n# If necessary, remove rows with missing values in other variables\nnew_data &lt;- drop_na(data)\n\n#normalize numerical variable\nnew_data &lt;- new_data %&gt;%\n  mutate(nightlyrank = scale(nightlyrank))\n\nHere I fix the issue we see in the visualization of 2 thursdays and wednesdays\n\nlibrary(stringr)\nnew_data &lt;- new_data %&gt;%\n  mutate(timeslot_et = str_squish(timeslot_et))\n\n\nggplot(new_data, aes(x = timeslot_et, y = viewers_in_millions)) +\n  geom_boxplot() + \n  ggtitle(\"Box Plot of Viewers in Millions by Time Slot (ET)\")+\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nAfter clearing the rows with missing numerical variables, it appears a whole day of the week is taken out of the data, there seems to still be a relationship between viewership and the time of week, however I am probably going to throw this predictor out due to the lack of variables.\n\nselected_col &lt;- c(\"original_network\", \"weekrank\", \"finals_venue\", \"nightlyrank\", \"viewers_in_millions\")\n\nnew_data &lt;- data_combined %&gt;%\n  select(all_of(selected_col))\n\nModel Training\n\nlibrary(rsample)\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n# split data\nset.seed(123) \nsplit &lt;- initial_split(new_data, prop = 0.7)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\nsum(is.na(train_data$viewers_in_millions))\n\n[1] 76\n\n# Remove rows with missing values in the response variable\ntrain_data &lt;- train_data %&gt;%\n  filter(!is.na(viewers_in_millions))\n# Impute missing values in the response variable with the mean\nmean_value &lt;- mean(train_data$viewers_in_millions, na.rm = TRUE)\ntrain_data$viewers_in_millions[is.na(train_data$viewers_in_millions)] &lt;- mean_value\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.2.1     ✔ tibble       3.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.2     ✔ yardstick    1.3.1\n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ randomForest::combine()  masks dplyr::combine()\n✖ purrr::discard()         masks scales::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ dplyr::lag()             masks stats::lag()\n✖ purrr::lift()            masks caret::lift()\n✖ randomForest::margin()   masks ggplot2::margin()\n✖ yardstick::precision()   masks caret::precision()\n✖ yardstick::recall()      masks caret::recall()\n✖ yardstick::sensitivity() masks caret::sensitivity()\n✖ yardstick::specificity() masks caret::specificity()\n✖ recipes::step()          masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n# Define the recipe with step_zv() to remove zero-variance columns\nrecipe &lt;- recipe(viewers_in_millions ~ ., data = train_data) %&gt;%\n  step_unknown(all_nominal(), -all_outcomes()) %&gt;%  # Handle unknown levels\n  step_impute_mode(all_nominal(), -all_outcomes()) %&gt;%  # Impute missing values in categorical variables\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%  # Convert categorical variables to dummy variables\n  step_zv(all_predictors()) %&gt;%  # Remove zero-variance columns\n  step_impute_mean(all_numeric(), -all_outcomes()) %&gt;%  # Impute missing values in numerical variables\n  step_normalize(all_numeric(), -all_outcomes())  # Normalize numerical variables\n\n# Check for missing values in the training data\nsum(is.na(train_data))\n\n[1] 19426\n\n\n3 Models I want to use and the workflow for them\n\n# Linear Model\nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Decision Tree\ndecision_tree_model &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# Random Forest\nrandom_forest_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n\n\n# Re-create the workflows with the updated recipe\nlinear_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(linear_model)\n\ndecision_tree_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(decision_tree_model)\n\nrandom_forest_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(random_forest_model)\n\n# Fit the models\nlinear_fit &lt;- fit(linear_workflow, data = train_data)\ndecision_tree_fit &lt;- fit(decision_tree_workflow, data = train_data)\nrandom_forest_fit &lt;- fit(random_forest_workflow, data = train_data)\n\nCross Validation\n\n# Define cross-validation folds\ncv_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Evaluate Linear Model\nlinear_results &lt;- fit_resamples(linear_workflow, resamples = cv_folds)\n\n→ A | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x5\n\n\n\n\nlinear_metrics &lt;- collect_metrics(linear_results)\n\n# Evaluate Decision Tree\ndecision_tree_results &lt;- fit_resamples(decision_tree_workflow, resamples = cv_folds)\n\nWarning: package 'rpart' was built under R version 4.3.3\n\ndecision_tree_metrics &lt;- collect_metrics(decision_tree_results)\n\n# Evaluate Random Forest\nrandom_forest_results &lt;- fit_resamples(random_forest_workflow, resamples = cv_folds)\n\nWarning: package 'ranger' was built under R version 4.3.3\n\nrandom_forest_metrics &lt;- collect_metrics(random_forest_results)\n\n\n# Print metrics for comparison\nprint(linear_metrics)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3.34      5 0.0171  Preprocessor1_Model1\n2 rsq     standard   0.753     5 0.00332 Preprocessor1_Model1\n\nprint(decision_tree_metrics)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3.71      5 0.0161  Preprocessor1_Model1\n2 rsq     standard   0.694     5 0.00393 Preprocessor1_Model1\n\nprint(random_forest_metrics)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   3.44      5 0.00992 Preprocessor1_Model1\n2 rsq     standard   0.746     5 0.00258 Preprocessor1_Model1\n\n\nBased on the metrics printed above, the model that I will be chosing is the linear model due to it having the highest rsq and lowest rmse.\nModel Fit\n\nfinal_model_fit &lt;- fit(linear_workflow, data = train_data)\n# Predict on test data\ntest_predictions &lt;- predict(final_model_fit, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\", : prediction from rank-deficient fit; consider predict(.,\nrankdeficient=\"NA\")\n\n# Evaluate performance on test data\ntest_metrics &lt;- metrics(test_predictions, truth = viewers_in_millions, estimate = .pred)\nprint(test_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       3.32 \n2 rsq     standard       0.762\n3 mae     standard       2.35 \n\n\n\nlibrary(ggplot2)\n\n# Combine metrics for plotting\nmetrics_combined &lt;- bind_rows(\n  linear_metrics %&gt;% mutate(model = \"Linear Model\"),\n  decision_tree_metrics %&gt;% mutate(model = \"Decision Tree\"),\n  random_forest_metrics %&gt;% mutate(model = \"Random Forest\")\n)\n\n# Plot performance metrics\nggplot(metrics_combined, aes(x = model, y = mean, fill = .metric)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Model Performance Comparison\", x = \"Model\", y = \"Metric Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen chosing the final model, it was a close call, however it is obvious when looking at the rmse column that the linear model was the best fitted model of the 3 I chose to test.\n\n# Visualization of predictions vs actual values\nggplot(test_predictions, aes(x = viewers_in_millions, y = .pred)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(title = \"Actual vs Predicted Viewers\", x = \"Actual Viewers (in millions)\", y = \"Predicted Viewers (in millions)\") +\n  theme_minimal()\n\nWarning: Removed 42 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs shown in the plot above, the prediction line roughly matches with the data, making this a good model to use when predicting viewers, however I think there is still some work to be done, whether it be recollecting data or me looking harder for other predictors to make this model improve."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nWarning: package 'here' was built under R version 4.3.3\n\n\nWarning: package 'knitr' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nBlood type\n0\n1\n2\n3\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n39.88889\n28.29507\n11\n15\n33\n64\n85\n▇▂▂▃▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\nFigure 2 shows that there is no correlation between height and weight.\n\n\n\n\n\n\n\n\nFigure 2: Age and weight scatterplot.\n\n\n\n\n\nFigure 3 appears to show that 3 of the 4 bloodtypes show normal(ish) distributions while O positive is skewed to the right.\n\n\n\n\n\n\n\n\nFigure 3: Age and weight scatterplot."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Administrator/Documents/GitHub/AidenSutherland-P2-portfolio/AidenSutherland-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                   `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                   &lt;chr&gt;                 \n1 Height          height in centimeters                   numeric value &gt;0 or NA\n2 Weight          weight in kilograms                     numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other)   M/F/O/NA              \n4 Age             age in years                            numeric value &gt;0 or NA\n5 Blood Type      Blood Type(A positive, A negative, etc) A+/A-, B+/B-, AB+/AB-…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height       &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"…\n$ Weight       &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender       &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"…\n$ Age          &lt;dbl&gt; 15, 64, 8, 12, 31, 85, 70, 11, 59, 21, 96, 33, 51, 18\n$ `Blood type` &lt;chr&gt; \"O+\", \"AB-\", \"O-\", \"B+\", \"AB+\", \"O+\", \"A-\", \"O+\", \"A+\", \"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   : 8.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:15.75  \n Mode  :character   Median :  70.0   Mode  :character   Median :32.00  \n                    Mean   : 602.7                      Mean   :41.00  \n                    3rd Qu.:  90.0                      3rd Qu.:62.75  \n                    Max.   :7000.0                      Max.   :96.00  \n                    NA's   :1                                          \n  Blood type       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age `Blood type`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M         15 O+          \n2 175        70 O         64 AB-         \n3 sixty      60 F          8 O-          \n4 178        76 F         12 B+          \n5 192        90 NA        31 AB+         \n6 6          55 F         85 O+          \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood type\n0\n1\n2\n3\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.00\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n41.00\n29.55\n8\n15.75\n32\n62.75\n96\n▇▂▂▂▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood type\n0\n1\n2\n3\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n43.54\n29.13\n11\n18.00\n33\n64\n96\n▇▃▃▃▃\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood type\n0\n1\n2\n3\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n43.54\n29.13\n11\n18.00\n33\n64\n96\n▇▃▃▃▃\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nBlood type\n0\n1\n2\n3\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n40.82\n26.15\n11\n16.5\n33\n61.5\n85\n▇▃▂▆▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nBlood type\n0\n1\n2\n3\n0\n7\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n40.82\n26.15\n11\n16.5\n33\n61.5\n85\n▇▃▂▆▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nBlood type\n0\n1\n2\n3\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n39.89\n28.30\n11\n15\n33\n64\n85\n▇▂▂▃▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Administrator/Documents/GitHub/AidenSutherland-P2-portfolio/AidenSutherland-P2-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Blood type            0             1   2   3     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  39.9 28.3  11  15  33  64   85 ▇▂▂▃▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nHere is box plot comparing blood type to height\n\np5 &lt;- ggplot(mydata, aes(x = `Blood type`, y = Height)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Height by Blood Type\",\n       x = \"Blood Type\",\n       y = \"Height\") \n \nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-blood-boxplot.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\nHere is a Scatter plot showing Age and Weight\n\np6 &lt;- ggplot(mydata, aes(x = Weight, y = Age)) +\n  geom_point() +\n  labs(title = \"Scatterplot of Age vs Weight\",\n       x = \"Weight\",\n       y = \"Age\") \nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Age-weight-scatter.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Here is the link to the news article that has the visualization I will be recreating: https://fivethirtyeight.com/features/aging-congress-boomers/\n\ndata &lt;- read.csv(\"~/GitHub/AidenSutherland-P2-portfolio/AidenSutherland-P2-portfolio/presentation-exercise/data_aging_congress.csv\")\nhead(data)\n\n  congress start_date chamber state_abbrev party_code                 bioname\n1       82 1951-01-03   House           ND        200    AANDAHL, Fred George\n2       80 1947-01-03   House           VA        100 ABBITT, Watkins Moorman\n3       81 1949-01-03   House           VA        100 ABBITT, Watkins Moorman\n4       82 1951-01-03   House           VA        100 ABBITT, Watkins Moorman\n5       83 1953-01-03   House           VA        100 ABBITT, Watkins Moorman\n6       84 1955-01-03   House           VA        100 ABBITT, Watkins Moorman\n  bioguide_id   birthday cmltv_cong cmltv_chamber age_days age_years generation\n1     A000001 1897-04-09          1             1    19626  53.73306       Lost\n2     A000002 1908-05-21          1             1    14106  38.62012   Greatest\n3     A000002 1908-05-21          2             2    14837  40.62149   Greatest\n4     A000002 1908-05-21          3             3    15567  42.62012   Greatest\n5     A000002 1908-05-21          4             4    16298  44.62149   Greatest\n6     A000002 1908-05-21          5             5    17028  46.62012   Greatest\n\n\nHere is the code to recreate the “Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023” graph found in the article, I used AI to recreate the overall graph, however I needed to manipulate the data somewhat in order for the graph to mimic what was on the website.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(plotly)\n\nWarning: package 'plotly' was built under R version 4.3.3\n\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndata &lt;- data %&gt;%\n  mutate(start_date = as.Date(start_date)) %&gt;%\n  filter(year(start_date) &gt;= 1919 & year(start_date) &lt;= 2023)\n\n# Extract the year from start_date\ndata &lt;- data %&gt;%\n  mutate(year = year(start_date))\n\n# Group by year and chamber, and calculate the median age\nmedian_age &lt;- data %&gt;%\n  group_by(year, chamber) %&gt;%\n  summarize(median_age = round(median(age_years, na.rm = TRUE)))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\nmedian_age &lt;- median_age %&gt;%\n  mutate(hover_text = paste(chamber, '&lt;br&gt;', year, '&lt;br&gt;', round(median_age, 1)))\n# Create an interactive plot\nplot &lt;- plot_ly(median_age, x = ~year, y = ~median_age, color = ~chamber, text = ~hover_text, hoverinfo = 'text', type = 'scatter', mode = 'markers', line = list(shape = 'hv', width = 6)) %&gt;%\n  layout(title = list(text = 'Median Age of the U.S. Senate and U.S. House by Year (1919-2023)', x = 0.5),\n         xaxis = list(title = 'Year', tickformat = \"%Y\", tickmode = 'linear', dtick = 10, range = c(1919, 2020), showgrid = FALSE),\n         yaxis = list(title = 'Median Age', range = c(45, 65), showgrid = TRUE),\n         legend = list(x = 0, y = 1, xanchor = 'left', yanchor = 'top', title = list(text = ''), traceorder = 'reversed'),\n         plot_bgcolor = 'white',\n         paper_bgcolor = 'white',\n         hoverdistance = 100,\n         spikedistance = -1\n         )\n\n# Show the plot\nplot\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\n\nWarning in RColorBrewer::brewer.pal(N, \"Set2\"): minimal value for n is 3, returning requested palette with 3 different levels\n\n\nA line object has been specified, but lines is not in the mode\nAdding lines to the mode...\n\n\nA line object has been specified, but lines is not in the mode\nAdding lines to the mode...\n\n\n\n\n\n\nThis next part I will be generating 2 tables to showing the median age for each Chamber of Congress from 1919 - 2023 I had to remove a few columns in the data frame I used for the 2 tables, specifically the chamber column given that each table would have a column that would say the same chamber over and over again.\n\nlibrary(flextable)\n\nWarning: package 'flextable' was built under R version 4.3.3\n\n\n\nAttaching package: 'flextable'\n\n\nThe following objects are masked from 'package:plotly':\n\n    highlight, style\n\n# Remove the hover_text column\nmedian_age &lt;- median_age[, !(names(median_age) %in% \"hover_text\")]\n\n# Filter data for Senate and House\nsenate_data &lt;- median_age[median_age$chamber == \"Senate\", ]\nhouse_data &lt;- median_age[median_age$chamber == \"House\", ]\n\ncols_to_include &lt;- setdiff(colnames(senate_data), \"chamber\")\n# Create flextable objects\nsenate_table &lt;- flextable(senate_data, cols_to_include)\nhouse_table &lt;- flextable(house_data, cols_to_include)\n\n# Define table properties\nsenate_table &lt;- set_table_properties(senate_table, width = 1, layout = \"autofit\")\nhouse_table &lt;- set_table_properties(house_table, width = 1, layout = \"autofit\")\n\nsenate_table &lt;- set_formatter(senate_table, year = function(x) format(x, big.mark = \"\", scientific = FALSE))\nhouse_table &lt;- set_formatter(house_table, year = function(x) format(x, big.mark = \"\", scientific = FALSE))\n\nsenate_table &lt;- add_header_row( senate_table, values = c(\"Median Age in The Senate from 1919 to 2023\"), colwidths = 2)\nhouse_table &lt;- add_header_row( house_table, values = c(\"Median Age in The House of Representatives from 1919 to 2023\"), colwidths = 2)\n\nsenate_table &lt;- theme_vader(senate_table)\nhouse_table &lt;- theme_vader(house_table)\nsenate_table\n\nMedian Age in The Senate from 1919 to 2023yearmedian_age191957192157192359192558192758192958193157193357193558193759193960194158194359194557194756194956195155195357195558195758195957196158196358196559196759196956197156197355197555197753197953198152198353198554198754198955199157199357199557199756199957200158200359200561200763200962201161201362201561201763201963202164202365\n\nhouse_table\n\nMedian Age in The House of Representatives from 1919 to 2023yearmedian_age191950192151192351192553192754192954193155193353193551193751193950194150194351194552194751194951195152195352195552195753195951196152196351196550196751196952197152197351197550197750197949198149198348198549198750198950199152199351199551199751199952200154200355200556200756200957201157201357201558201758201958202158202358"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "###Option 2\n#Generate the Sythetic Data For this I will do Age, salary, and employee_type for my variables\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\nset.seed(123)\nn &lt;- 1000\n\n# Continuous variables\nx1 &lt;- rnorm(n, mean = 50, sd = 10)  \nx2 &lt;- rnorm(n, mean = 30, sd = 5)   \n\n# Categorical variable with 3 levels\nx3 &lt;- sample(c(\"A\", \"B\", \"C\"), n, replace = TRUE)\n\n# Dependent variable with some associations\ny &lt;- 3 + 2*x1 + 0.5*x2 + ifelse(x3 == \"A\", 5, ifelse(x3 == \"B\", -3, 0)) + rnorm(n)\n\ndata &lt;- data.frame(x1, x2, x3, y)\nhead(data)\n\n        x1       x2 x3        y\n1 44.39524 25.02101  B 101.7480\n2 47.69823 24.80022  B 109.0266\n3 65.58708 29.91010  A 154.1894\n4 50.70508 29.33912  C 117.1397\n5 51.29288 17.25329  C 114.2172\n6 67.15065 35.20287  A 158.7035\n\n\n#Explore the data\n\nsummary(data)\n\n       x1              x2             x3                  y         \n Min.   :21.90   Min.   :14.76   Length:1000        Min.   : 54.43  \n 1st Qu.:43.72   1st Qu.:26.73   Class :character   1st Qu.:105.38  \n Median :50.09   Median :30.27   Mode  :character   Median :118.34  \n Mean   :50.16   Mean   :30.21                      Mean   :118.83  \n 3rd Qu.:56.65   3rd Qu.:33.77                      3rd Qu.:132.92  \n Max.   :82.41   Max.   :46.95                      Max.   :182.24  \n\n\n\nggplot(data, aes(y)) +\n  geom_histogram(binwidth = 10, fill = 'blue', color = 'black', alpha = 0.7) +\n  ggtitle('Histogram of y')\n\n\n\n\n\n\n\nggplot(data, aes(x = x1, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", col = \"red\") +\n  ggtitle('Scatter plot of y vs x1')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data, aes(x = x2, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", col = \"red\") +\n  ggtitle('Scatter plot of y vs x2')\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot(data, aes(x = x3, y = y)) +\n  geom_boxplot(fill = 'cyan', color = 'black') +\n  ggtitle('Boxplot of y vs x3')\n\n\n\n\n\n\n\n\n#Simple models for generated data\n\n#Linear Models\nmodel1 &lt;- lm(y ~ x1 + x2 + x3, data = data)\n\nmodel2 &lt;- lm(y ~ x1 * x3 + x2, data = data)\n\n# Logistic regression (create a binary outcome variable)\ndata$y_bin &lt;- ifelse(data$y &gt; median(data$y), 1, 0)\nmodel3 &lt;- glm(y_bin ~ x1 + x2 + x3, data = data, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# Print model summaries to compare\nsummary(model1)\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x3, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0739 -0.6416 -0.0409  0.6742  3.4319 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.418014   0.249278   33.77   &lt;2e-16 ***\nx1           1.996237   0.003245  615.21   &lt;2e-16 ***\nx2           0.492096   0.006380   77.13   &lt;2e-16 ***\nx3B         -8.026385   0.077186 -103.99   &lt;2e-16 ***\nx3C         -4.959683   0.081739  -60.68   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.013 on 995 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9976 \nF-statistic: 1.02e+05 on 4 and 995 DF,  p-value: &lt; 2.2e-16\n\nsummary(model2)\n\n\nCall:\nlm(formula = y ~ x1 * x3 + x2, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0947 -0.6326 -0.0443  0.6634  3.3983 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.727819   0.352872  24.734   &lt;2e-16 ***\nx1           1.990097   0.005935 335.300   &lt;2e-16 ***\nx3B         -8.370087   0.400964 -20.875   &lt;2e-16 ***\nx3C         -5.520303   0.425525 -12.973   &lt;2e-16 ***\nx2           0.492039   0.006381  77.111   &lt;2e-16 ***\nx1:x3B       0.006851   0.007852   0.873    0.383    \nx1:x3C       0.011159   0.008311   1.343    0.180    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.013 on 993 degrees of freedom\nMultiple R-squared:  0.9976,    Adjusted R-squared:  0.9976 \nF-statistic: 6.798e+04 on 6 and 993 DF,  p-value: &lt; 2.2e-16\n\nsummary(model3)\n\n\nCall:\nglm(formula = y_bin ~ x1 + x2 + x3, family = binomial, data = data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -280.3656    56.7476  -4.941 7.79e-07 ***\nx1             5.0907     1.0289   4.948 7.50e-07 ***\nx2             1.2505     0.2625   4.764 1.90e-06 ***\nx3B          -21.4205     4.4475  -4.816 1.46e-06 ***\nx3C          -13.2524     2.8114  -4.714 2.43e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1386.29  on 999  degrees of freedom\nResidual deviance:   60.46  on 995  degrees of freedom\nAIC: 70.46\n\nNumber of Fisher Scoring iterations: 13\n\n\nModel 1 shows the main linear associations, while model 2 shows both main and interaction effects. Model 3 demonstrates the flexibility of logistic regression in handling binary outcomes with the same associations in the previous 2 models."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "The data set Provisional_COVID-19_Deaths_by_Sex_and_Age_20240703.csv shows data of deaths involving COVID-19, pneumonia, and influenza reported to NCHS by sex, age group, and jurisdiction of occurrence.It was last updated on September 27th, 2023.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(stringr)\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nHere is a quick preview of what the data set looks like after importing it.\n\ncovid_data &lt;- read.csv(\"~/GitHub/AidenSutherland-P2-portfolio/AidenSutherland-P2-portfolio/cdcdata-exercise/Provisional_COVID-19_Deaths_by_Sex_and_Age_20240703.csv\")\nhead(covid_data)\n\n  Data.As.Of Start.Date   End.Date    Group Year Month         State       Sex\n1 09/27/2023 01/01/2020 09/23/2023 By Total   NA    NA United States All Sexes\n2 09/27/2023 01/01/2020 09/23/2023 By Total   NA    NA United States All Sexes\n3 09/27/2023 01/01/2020 09/23/2023 By Total   NA    NA United States All Sexes\n4 09/27/2023 01/01/2020 09/23/2023 By Total   NA    NA United States All Sexes\n5 09/27/2023 01/01/2020 09/23/2023 By Total   NA    NA United States All Sexes\n6 09/27/2023 01/01/2020 09/23/2023 By Total   NA    NA United States All Sexes\n     Age.Group COVID.19.Deaths Total.Deaths Pneumonia.Deaths\n1     All Ages         1146774     12303399          1162844\n2 Under 1 year             519        73213             1056\n3   0-17 years            1696       130970             2961\n4    1-4 years             285        14299              692\n5   5-14 years             509        22008              818\n6  15-24 years            3021       133459             3175\n  Pneumonia.and.COVID.19.Deaths Influenza.Deaths\n1                        569264            22229\n2                            95               64\n3                           424              509\n4                            66              177\n5                           143              219\n6                          1257              206\n  Pneumonia..Influenza..or.COVID.19.Deaths Footnote\n1                                  1760095         \n2                                     1541         \n3                                     4716         \n4                                     1079         \n5                                     1390         \n6                                     5133         \n\n\nHere i changed the columns to be separated with _ instead of . as well as removing any rows containing “NA” as well as removing the footnote column and any row containing the United States instead of the actual state in the “state” column.\nThe Age_Group column was a mess, so I filtered out the data that seemed to have different age group increments so we only have one age group to go with.\n\ncolnames(covid_data) &lt;- str_replace_all(colnames(covid_data), \"[^[:alnum:]_]\", \"_\")\ncovid_data &lt;- na.omit(covid_data)\ncovid_data &lt;- covid_data %&gt;%\n  select(-Footnote)\ncovid_data &lt;- covid_data %&gt;%\n  filter(State != \"United States\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"0-17 years\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"18-29 years\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"30-39 years\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"40-49 years\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"50-59 years\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"50-64 years\")\ncovid_data &lt;- covid_data %&gt;%\n  filter(Age_Group != \"All Ages\")\nsummary(covid_data)\n\n  Data_As_Of         Start_Date          End_Date            Group          \n Length:21496       Length:21496       Length:21496       Length:21496      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n      Year          Month           State               Sex           \n Min.   :2020   Min.   : 1.000   Length:21496       Length:21496      \n 1st Qu.:2020   1st Qu.: 4.000   Class :character   Class :character  \n Median :2021   Median : 7.000   Mode  :character   Mode  :character  \n Mean   :2021   Mean   : 6.537                                        \n 3rd Qu.:2022   3rd Qu.: 9.000                                        \n Max.   :2023   Max.   :12.000                                        \n  Age_Group         COVID_19_Deaths    Total_Deaths    Pneumonia_Deaths \n Length:21496       Min.   :   0.00   Min.   :   0.0   Min.   :   0.00  \n Class :character   1st Qu.:   0.00   1st Qu.:  15.0   1st Qu.:   0.00  \n Mode  :character   Median :  16.00   Median : 131.0   Median :  16.00  \n                    Mean   :  51.12   Mean   : 460.3   Mean   :  47.75  \n                    3rd Qu.:  57.00   3rd Qu.: 634.2   3rd Qu.:  63.00  \n                    Max.   :3949.00   Max.   :9932.0   Max.   :1824.00  \n Pneumonia_and_COVID_19_Deaths Influenza_Deaths  \n Min.   :   0.00               Min.   :  0.0000  \n 1st Qu.:   0.00               1st Qu.:  0.0000  \n Median :  10.00               Median :  0.0000  \n Mean   :  25.61               Mean   :  0.7475  \n 3rd Qu.:  27.00               3rd Qu.:  0.0000  \n Max.   :1576.00               Max.   :177.0000  \n Pneumonia__Influenza__or_COVID_19_Deaths\n Min.   :   0.0                          \n 1st Qu.:   0.0                          \n Median :  25.0                          \n Mean   :  73.9                          \n 3rd Qu.:  94.0                          \n Max.   :4198.0                          \n\n\nHere are the columns I selected for the final data set.\n\nselected_columns &lt;- c(\"Year\", \"Month\", \"State\", \"Sex\", \"Age_Group\", \"COVID_19_Deaths\", \"Total_Deaths\", \"Pneumonia_Deaths\", \"Pneumonia_and_COVID_19_Deaths\", \"Influenza_Deaths\", \"Pneumonia__Influenza__or_COVID_19_Deaths\")\ncovid_data_subset &lt;- covid_data %&gt;%\n  select(all_of(selected_columns))\n\n##EDA\nHere is a summary of all the categorical variables I realize that NYC is in New York, however the population density of that city should be noted separately from other reports from the rest of the state.\n\ncat_vars &lt;- c(\"Year\", \"Month\", \"State\", \"Sex\", \"Age_Group\")\n\nfor (cat_var in cat_vars) {\n  cat_var_summary &lt;- covid_data_subset %&gt;%\n    group_by(!!sym(cat_var)) %&gt;%\n    summarise(count = n()) %&gt;%\n    mutate(percentage = round(count / sum(count) * 100, 2))\n  \n  print(cat_var_summary)\n}\n\n# A tibble: 4 × 3\n   Year count percentage\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1  2020  6308       29.3\n2  2021  7713       35.9\n3  2022  4408       20.5\n4  2023  3067       14.3\n# A tibble: 12 × 3\n   Month count percentage\n   &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n 1     1  1920       8.93\n 2     2  1778       8.27\n 3     3  1605       7.47\n 4     4  1566       7.29\n 5     5  1667       7.75\n 6     6  1713       7.97\n 7     7  1866       8.68\n 8     8  2130       9.91\n 9     9  2202      10.2 \n10    10  1763       8.2 \n11    11  1634       7.6 \n12    12  1652       7.69\n# A tibble: 53 × 3\n   State                count percentage\n   &lt;chr&gt;                &lt;int&gt;      &lt;dbl&gt;\n 1 Alabama                394       1.83\n 2 Alaska                 360       1.67\n 3 Arizona                499       2.32\n 4 Arkansas               384       1.79\n 5 California             551       2.56\n 6 Colorado               407       1.89\n 7 Connecticut            415       1.93\n 8 Delaware               356       1.66\n 9 District of Columbia   286       1.33\n10 Florida                434       2.02\n# ℹ 43 more rows\n# A tibble: 3 × 3\n  Sex       count percentage\n  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 All Sexes  7477       34.8\n2 Female     6664       31  \n3 Male       7355       34.2\n# A tibble: 11 × 3\n   Age_Group         count percentage\n   &lt;chr&gt;             &lt;int&gt;      &lt;dbl&gt;\n 1 1-4 years          1742       8.1 \n 2 15-24 years        1939       9.02\n 3 25-34 years        1346       6.26\n 4 35-44 years        1260       5.86\n 5 45-54 years        1657       7.71\n 6 5-14 years         1482       6.89\n 7 55-64 years        2016       9.38\n 8 65-74 years        2347      10.9 \n 9 75-84 years        2457      11.4 \n10 85 years and over  2485      11.6 \n11 Under 1 year       2765      12.9 \n\n\nHere is a summary of all of the continuous variables.\n\ncont_vars &lt;- c(\"COVID_19_Deaths\", \"Total_Deaths\", \"Pneumonia_Deaths\", \"Pneumonia_and_COVID_19_Deaths\", \"Influenza_Deaths\", \"Pneumonia__Influenza__or_COVID_19_Deaths\")\n\nfor (cont_var in cont_vars) {\n  cont_var_summary &lt;- covid_data_subset %&gt;%\n    summarise(mean = mean(!!sym(cont_var), na.rm = TRUE),\n              sd = sd(!!sym(cont_var), na.rm = TRUE))\n  \n  print(cont_var_summary)\n}\n\n      mean       sd\n1 51.11816 120.4679\n      mean       sd\n1 460.3108 761.6771\n      mean       sd\n1 47.75158 89.33372\n      mean       sd\n1 25.60993 60.04298\n       mean       sd\n1 0.7474879 5.008657\n      mean       sd\n1 73.90217 147.5101\n\n\nHere are corresponding plots for each continuous variable to check for normality.\n\nfor (cont_var in cont_vars) {\n hist_plot &lt;- ggplot(covid_data_subset, aes_string(x = cont_var)) +\n    geom_histogram(aes(y = ..density..), binwidth = 100, fill = \"blue\", alpha = 0.7, color = \"black\") +\n    stat_function(fun = dnorm, args = list(mean = mean(covid_data_subset[[cont_var]], na.rm = TRUE), \n                                           sd = sd(covid_data_subset[[cont_var]], na.rm = TRUE)), \n                  color = \"red\", size = 1) +\n    labs(title = paste(\"Histogram and Normal Curve of\", cont_var),\n         x = cont_var,\n         y = \"Density\")\n print(hist_plot)\n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is A Q-Q plot\n\nfor (cont_var in cont_vars) {\n  qq_plot &lt;- ggplot(covid_data_subset, aes_string(sample = cont_var)) +\n    stat_qq() +\n    stat_qq_line() +\n    labs(title = paste(\"Q-Q Plot of\", cont_var),\n         x = \"Theoretical Quantiles\",\n         y = \"Sample Quantiles\")\n  \n  # Arrange the plots side by side\n  grid.arrange(hist_plot, qq_plot, ncol = 2)\n  }"
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "Background",
    "text": "Background\nMy name is Aiden Sutherland, I am 22 years old and am currently pursuing a MS in Data Analytics at UTSA. I have grown up in San Antonio for most of my life, however my dad was stationed in Hawaii for about 3 and a half years when I was in elementary schools, which started my lifelong addiction to Hawaiian food. I have a younger sister and a twin brother, one day ill send him to class in my stead and y’all will never know. I am kind of a military brat, which (if you have lived in San Antonio for long) isn’t uncommon. I enjoy playing video games, going to any body of water, and eating lots of amazing food. As of right now I am in the process of accruing certifications and waiting to join the USAF as an officer."
  },
  {
    "objectID": "aboutme.html#investor-sentiment",
    "href": "aboutme.html#investor-sentiment",
    "title": "About me",
    "section": "Investor Sentiment",
    "text": "Investor Sentiment\nhttps://www.aaii.com/sentimentsurvey\nAbove is a link to the American Association of Individual Investors. I know that I cannot hear enough about “tHe EcoNomY iS gOiN tO HeLl” from people who don’t invest anything and only look at gas prices, so here is a better view of what most people who are actually invested in the economy think.It also breaks down where these people get their investment income from."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "load the dslabs and ggplot2 library\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nhelp for gapminder and overview of data structure\n\nhelp(gapminder)\n\nstarting httpd help server ... done\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n\nSummary of the gapminder data\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\nWhat type of object is gapminder\n\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nFilter out the data to include only African Countries and check the structure of the new data set.\n\nafricadata &lt;- gapminder[gapminder$continent == \"Africa\", ]\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\ncreating 2 new objects from the africadata data set\n\nafrica_infant_life &lt;- africadata[, c(\"infant_mortality\", \"life_expectancy\")]\nafrica_pop_life &lt;- africadata[, c(\"population\", \"life_expectancy\")]\n\nScatter plot for life expectancy as a function of infant mortality\n\nggplot(africa_infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality in African Countries\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\") \n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nScatter plot for life expectancy as a function of population size\n\nggplot(africa_pop_life, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size in African Countries\",\n       x = \"Population (log scale)\",\n       y = \"Life Expectancy\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nreferring back to the African data set to observe what is happening.\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n\nThe streaks of data can be a result of there being different years for the same countries, which could be a reason why you see streaks in different areas of the map, as it could be plotting the same country year by year.\nCheck for missing values in infant_mortality by year\n\nmissing_infant_mortality &lt;- aggregate(infant_mortality ~ year, data = africadata, function(x) sum(is.na(x)))\nprint(missing_infant_mortality)\n\n   year infant_mortality\n1  1960                0\n2  1961                0\n3  1962                0\n4  1963                0\n5  1964                0\n6  1965                0\n7  1966                0\n8  1967                0\n9  1968                0\n10 1969                0\n11 1970                0\n12 1971                0\n13 1972                0\n14 1973                0\n15 1974                0\n16 1975                0\n17 1976                0\n18 1977                0\n19 1978                0\n20 1979                0\n21 1980                0\n22 1981                0\n23 1982                0\n24 1983                0\n25 1984                0\n26 1985                0\n27 1986                0\n28 1987                0\n29 1988                0\n30 1989                0\n31 1990                0\n32 1991                0\n33 1992                0\n34 1993                0\n35 1994                0\n36 1995                0\n37 1996                0\n38 1997                0\n39 1998                0\n40 1999                0\n41 2000                0\n42 2001                0\n43 2002                0\n44 2003                0\n45 2004                0\n46 2005                0\n47 2006                0\n48 2007                0\n49 2008                0\n50 2009                0\n51 2010                0\n52 2011                0\n53 2012                0\n54 2013                0\n55 2014                0\n56 2015                0\n\n\nfilter for the year 2000 and display the structure and summary of the data\n\nafrica_2000 &lt;- africadata[africadata$year == 2000, ]\n\nstr(africa_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africa_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\nScatter plot of life expectancy vs. infant mortality for the year 2000\n\nggplot(africa_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality in African Countries (2000)\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\")\n\n\n\n\n\n\n\n\nScatter plot of life expectancy vs. population size for the year 2000\n\nggplot(africa_2000, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size in African Countries (2000)\",\n       x = \"Population (log scale)\",\n       y = \"Life Expectancy\")\n\n\n\n\n\n\n\n\nFit linear models\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africa_2000)\nfit2 &lt;- lm(life_expectancy ~ population, data = africa_2000)\n\nsummary of the fitted models\n\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africa_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africa_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nGiven that the p-value for the first fitted model (fit1) is extremely low, we can conclude that there is a significant negative relationship between infant mortality and life expectancy.\nGiven the p-value for the second fitted model (fit2) is high, we can conclude that there is no significant relationship between population size and life expectancy.\nThis section is from My other account :)\nLoad the death_prob dataset, which is a dataset that lists the probability of death within 1 year by age and sex in the United States in 2015. Also get an overview of the dataset structure\n\ndata(death_prob)\nclass(death_prob)\n\n[1] \"data.frame\"\n\nstr(death_prob)\n\n'data.frame':   240 obs. of  3 variables:\n $ age : int  0 1 2 3 4 5 6 7 8 9 ...\n $ sex : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ prob: num  0.006383 0.000453 0.000282 0.00023 0.000169 ...\n\n\nNext I will only be looking at the death probability of just women, so I will create a new dataset with only the “Feamale” variable. I also repeat the steps from death_pr to look at the new dataset structure\n\nwomen_death_prob &lt;- subset(death_prob, sex == \"Female\")\nstr(women_death_prob)\n\n'data.frame':   120 obs. of  3 variables:\n $ age : int  0 1 2 3 4 5 6 7 8 9 ...\n $ sex : Factor w/ 2 levels \"Female\",\"Male\": 1 1 1 1 1 1 1 1 1 1 ...\n $ prob: num  0.005374 0.000353 0.000231 0.000165 0.000129 ...\n\nsummary(women_death_prob)\n\n      age             sex           prob         \n Min.   :  0.00   Female:120   Min.   :0.000091  \n 1st Qu.: 29.75   Male  :  0   1st Qu.:0.000779  \n Median : 59.50                Median :0.006537  \n Mean   : 59.50                Mean   :0.121477  \n 3rd Qu.: 89.25                3rd Qu.:0.121924  \n Max.   :119.00                Max.   :0.899639  \n\n\nCreate new datasets from the women_depth_prob object.\n\nage_death_prob &lt;- death_prob[, c(\"age\", \"prob\")]\nsex_death_prob &lt;- death_prob[, c(\"sex\", \"prob\")]\n\ndisplay the new objects’ structure\n\nstr(age_death_prob)\n\n'data.frame':   240 obs. of  2 variables:\n $ age : int  0 1 2 3 4 5 6 7 8 9 ...\n $ prob: num  0.006383 0.000453 0.000282 0.00023 0.000169 ...\n\nstr(sex_death_prob)\n\n'data.frame':   240 obs. of  2 variables:\n $ sex : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 2 2 2 2 2 ...\n $ prob: num  0.006383 0.000453 0.000282 0.00023 0.000169 ...\n\n\nDisplay the summary of both objects’\n\nsummary(age_death_prob)\n\n      age              prob         \n Min.   :  0.00   Min.   :0.000091  \n 1st Qu.: 29.75   1st Qu.:0.001318  \n Median : 59.50   Median :0.008412  \n Mean   : 59.50   Mean   :0.127254  \n 3rd Qu.: 89.25   3rd Qu.:0.138332  \n Max.   :119.00   Max.   :0.899639  \n\nsummary(sex_death_prob)\n\n     sex           prob         \n Female:120   Min.   :0.000091  \n Male  :120   1st Qu.:0.001318  \n              Median :0.008412  \n              Mean   :0.127254  \n              3rd Qu.:0.138332  \n              Max.   :0.899639  \n\n\nPlot death_prob as a function of age\n\nggplot(age_death_prob, aes(x = age, y = prob)) +\n  geom_point() +\n  labs(title = \"Death Probability as a Function of Age\",\n       x = \"Age\",\n       y = \"Death Probability\")\n\n\n\n\n\n\n\n\nPlot death_prob as a function of sex\n\nggplot(sex_death_prob, aes(x = sex, y = prob)) +\n  geom_boxplot() +\n  labs(title = \"Death Probability as a Function of Sex\",\n       x = \"Sex\",\n       y = \"Death Probability\")\n\n\n\n\n\n\n\n\nFit linear model: death_prob as a function of age and the second model as a function of sex\n\nfit_age &lt;- lm(prob ~ age, data = age_death_prob)\nfit_sex &lt;- lm(prob ~ sex, data = sex_death_prob)\n\nHere is a Summary of the 2 new models\n\nsummary(fit_age)\n\n\nCall:\nlm(formula = prob ~ age, data = age_death_prob)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.18435 -0.12208 -0.02019  0.08843  0.48110 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.1640301  0.0186648  -8.788 3.08e-16 ***\nage          0.0048955  0.0002711  18.058  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1455 on 238 degrees of freedom\nMultiple R-squared:  0.5781,    Adjusted R-squared:  0.5763 \nF-statistic: 326.1 on 1 and 238 DF,  p-value: &lt; 2.2e-16\n\nsummary(fit_sex)\n\n\nCall:\nlm(formula = prob ~ sex, data = sex_death_prob)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.13294 -0.12178 -0.11685  0.01252  0.77816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.12148    0.02044   5.943 9.86e-09 ***\nsexMale      0.01155    0.02891   0.400     0.69    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2239 on 238 degrees of freedom\nMultiple R-squared:  0.0006709, Adjusted R-squared:  -0.003528 \nF-statistic: 0.1598 on 1 and 238 DF,  p-value: 0.6897\n\n\nThe p-value for age is extremely low, indicating that the relationship between age and death probability is statistically significant. The p-value for sex is not statistically significant, meaning there is no difference in death probability between men and women."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aiden’s website and data analysis portfolio",
    "section": "",
    "text": "Hello\n\nAnd hello again\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nHave fun!\n\nFeel free to change this text any way you want 😁!"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Aiden Sutherland's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  }
]